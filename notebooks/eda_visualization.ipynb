{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d7d805",
   "metadata": {},
   "source": [
    "# SDG Multi-Label Classification: EDA & Visualization\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a comprehensive **multi-label classification system** for predicting Sustainable Development Goals (SDGs) from scientific publication data using a **hybrid approach** combining clustering and classification techniques.\n",
    "\n",
    "### Dataset Description\n",
    "- **Total entries**: 7,147 scientific publications from Google Scholar\n",
    "- **Input features**: Title (text), Authors, Year, Cited count\n",
    "- **Target labels**: Multi-label SDG categories (e.g., \"SDG 3; SDG 9; SDG 13\")\n",
    "\n",
    "### Methodology\n",
    "1. **Data Preprocessing**: Handle missing values, text cleaning, feature normalization\n",
    "2. **Feature Engineering**: TF-IDF/BERT text vectorization + numerical features\n",
    "3. **Clustering Analysis**: K-Means/DBSCAN for semantic grouping\n",
    "4. **Hybrid Classification**: One-vs-Rest with ensemble methods + clustering features\n",
    "5. **Evaluation**: Multi-label metrics (Exact Match, Hamming Loss, F1-Score, Jaccard)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec03fa5",
   "metadata": {},
   "source": [
    "## 1. Import Libraries & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1cf8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    hamming_loss, jaccard_score, classification_report,\n",
    "    multilabel_confusion_matrix, silhouette_score\n",
    ")\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Plotting and visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791dce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "DATA_PATH = \"../data/2503_to_3336_preprocessing_labeling.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: File not found at {DATA_PATH}\")\n",
    "    print(\"Please make sure the data file exists in the correct location.\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Dataset Dimensions:\")\n",
    "print(f\"   Rows: {df.shape[0]:,}\")\n",
    "print(f\"   Columns: {df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nüìã Column Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(f\"\\nüîç First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"üîç Missing Values Analysis:\")\n",
    "print(\"=\"*40)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing Percentage': missing_percentages.values\n",
    "})\n",
    "\n",
    "print(missing_df)\n",
    "\n",
    "# Basic statistics for numerical columns\n",
    "print(\"\\nüìà Numerical Columns Statistics:\")\n",
    "print(\"=\"*40)\n",
    "print(df.describe())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nüè∑Ô∏è Data Types:\")\n",
    "print(\"=\"*40)\n",
    "print(df.dtypes)\n",
    "\n",
    "# Sample of the SDGs column to understand the format\n",
    "print(\"\\nüéØ Sample SDG Labels:\")\n",
    "print(\"=\"*40)\n",
    "for i, sdg in enumerate(df['SDGs'].head(10)):\n",
    "    print(f\"{i+1:2d}. {sdg}\")\n",
    "\n",
    "# Check unique values in categorical columns\n",
    "print(f\"\\nüìä Unique Years: {df['Year'].nunique()} (Range: {df['Year'].min()} - {df['Year'].max()})\")\n",
    "print(f\"üìä Unique Citation counts: {df['Cited'].nunique()} (Range: {df['Cited'].min()} - {df['Cited'].max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c04f2c",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "### Handle Missing Values & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83526176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"üõ†Ô∏è PREPROCESSING STEPS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Handle missing values\n",
    "print(\"\\n1Ô∏è‚É£ Handling Missing Values...\")\n",
    "\n",
    "# Fill missing Authors with 'Unknown'\n",
    "df_processed['Authors'] = df_processed['Authors'].fillna('Unknown')\n",
    "\n",
    "# Fill missing Title with empty string\n",
    "df_processed['Title'] = df_processed['Title'].fillna('')\n",
    "\n",
    "# Fill missing Year with median\n",
    "median_year = df_processed['Year'].median()\n",
    "df_processed['Year'] = df_processed['Year'].fillna(median_year)\n",
    "\n",
    "# Fill missing Cited with 0\n",
    "df_processed['Cited'] = df_processed['Cited'].fillna(0)\n",
    "\n",
    "print(f\"   ‚úÖ Authors: {df['Authors'].isnull().sum()} ‚Üí {df_processed['Authors'].isnull().sum()}\")\n",
    "print(f\"   ‚úÖ Title: {df['Title'].isnull().sum()} ‚Üí {df_processed['Title'].isnull().sum()}\")\n",
    "print(f\"   ‚úÖ Year: {df['Year'].isnull().sum()} ‚Üí {df_processed['Year'].isnull().sum()}\")\n",
    "print(f\"   ‚úÖ Cited: {df['Cited'].isnull().sum()} ‚Üí {df_processed['Cited'].isnull().sum()}\")\n",
    "\n",
    "# 2. Parse SDG labels\n",
    "print(\"\\n2Ô∏è‚É£ Parsing SDG Labels...\")\n",
    "\n",
    "def parse_sdgs(sdg_string):\n",
    "    \"\"\"Parse SDG labels from string format to list\"\"\"\n",
    "    if pd.isna(sdg_string):\n",
    "        return []\n",
    "    \n",
    "    # Extract SDG numbers using regex\n",
    "    sdg_matches = re.findall(r'SDG (\\d+)', str(sdg_string))\n",
    "    return [f\"SDG_{num}\" for num in sdg_matches]\n",
    "\n",
    "# Apply SDG parsing\n",
    "df_processed['SDG_labels'] = df_processed['SDGs'].apply(parse_sdgs)\n",
    "\n",
    "# Remove rows with no SDG labels\n",
    "initial_length = len(df_processed)\n",
    "df_processed = df_processed[df_processed['SDG_labels'].apply(len) > 0]\n",
    "final_length = len(df_processed)\n",
    "\n",
    "print(f\"   ‚úÖ Rows with valid SDG labels: {final_length:,} (removed {initial_length - final_length:,} rows)\")\n",
    "\n",
    "# 3. Clean and preprocess title text\n",
    "print(\"\\n3Ô∏è‚É£ Text Preprocessing...\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "df_processed['Title_cleaned'] = df_processed['Title'].apply(clean_text)\n",
    "\n",
    "print(f\"   ‚úÖ Text cleaning completed\")\n",
    "print(f\"   ‚úÖ Average title length: {df_processed['Title_cleaned'].str.len().mean():.1f} characters\")\n",
    "\n",
    "print(f\"\\nüìä Final processed dataset shape: {df_processed.shape}\")\n",
    "\n",
    "# Display processed sample\n",
    "print(\"\\nüîç Sample of processed data:\")\n",
    "df_processed[['Title_cleaned', 'Year', 'Cited', 'SDG_labels']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG Distribution Analysis\n",
    "print(\"üéØ SDG DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Extract all SDG labels\n",
    "all_sdgs = []\n",
    "for sdg_list in df_processed['SDG_labels']:\n",
    "    all_sdgs.extend(sdg_list)\n",
    "\n",
    "sdg_counts = pd.Series(all_sdgs).value_counts()\n",
    "print(f\"\\nüìà Total SDG instances: {len(all_sdgs)}\")\n",
    "print(f\"üìà Unique SDG classes: {len(sdg_counts)}\")\n",
    "print(f\"üìà Average SDGs per publication: {len(all_sdgs) / len(df_processed):.2f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Top 10 Most Common SDGs:\")\n",
    "print(sdg_counts.head(10))\n",
    "\n",
    "# Multi-label statistics\n",
    "sdg_per_doc = df_processed['SDG_labels'].apply(len)\n",
    "print(f\"\\nüìä Multi-label Statistics:\")\n",
    "print(f\"   Single label: {sum(sdg_per_doc == 1):,} ({sum(sdg_per_doc == 1)/len(df_processed)*100:.1f}%)\")\n",
    "print(f\"   Multiple labels: {sum(sdg_per_doc > 1):,} ({sum(sdg_per_doc > 1)/len(df_processed)*100:.1f}%)\")\n",
    "print(f\"   Max labels per doc: {sdg_per_doc.max()}\")\n",
    "print(f\"   Average labels per doc: {sdg_per_doc.mean():.2f}\")\n",
    "\n",
    "# Visualize SDG distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. SDG frequency bar plot\n",
    "axes[0, 0].bar(range(len(sdg_counts)), sdg_counts.values)\n",
    "axes[0, 0].set_title('SDG Label Frequency Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('SDG Classes')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Labels per document histogram\n",
    "axes[0, 1].hist(sdg_per_doc, bins=range(1, sdg_per_doc.max() + 2), alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Distribution of Labels per Document', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Number of SDG Labels')\n",
    "axes[0, 1].set_ylabel('Number of Documents')\n",
    "\n",
    "# 3. Year distribution\n",
    "axes[1, 0].hist(df_processed['Year'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Publication Year Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Year')\n",
    "axes[1, 0].set_ylabel('Number of Publications')\n",
    "\n",
    "# 4. Citation distribution (log scale)\n",
    "cited_nonzero = df_processed[df_processed['Cited'] > 0]['Cited']\n",
    "axes[1, 1].hist(np.log1p(cited_nonzero), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Citation Distribution (Log Scale)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Log(Citations + 1)')\n",
    "axes[1, 1].set_ylabel('Number of Publications')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top SDGs pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_10_sdgs = sdg_counts.head(10)\n",
    "plt.pie(top_10_sdgs.values, labels=top_10_sdgs.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Top 10 SDG Distribution', fontsize=16, fontweight='bold')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda99bd",
   "metadata": {},
   "source": [
    "## 3. Text Vectorization (TF-IDF & Word Analysis)\n",
    "\n",
    "### TF-IDF Vectorization and Text Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1cae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Analysis and Vectorization\n",
    "print(\"üìù TEXT ANALYSIS & VECTORIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Basic text statistics\n",
    "print(\"\\n1Ô∏è‚É£ Basic Text Statistics:\")\n",
    "title_lengths = df_processed['Title_cleaned'].str.len()\n",
    "title_words = df_processed['Title_cleaned'].str.split().str.len()\n",
    "\n",
    "print(f\"   Average title length: {title_lengths.mean():.1f} characters\")\n",
    "print(f\"   Average words per title: {title_words.mean():.1f}\")\n",
    "print(f\"   Max title length: {title_lengths.max()} characters\")\n",
    "print(f\"   Max words per title: {title_words.max()}\")\n",
    "\n",
    "# 2. Word frequency analysis\n",
    "print(\"\\n2Ô∏è‚É£ Word Frequency Analysis:\")\n",
    "all_words = ' '.join(df_processed['Title_cleaned']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "print(f\"   Total words: {len(all_words):,}\")\n",
    "print(f\"   Unique words: {len(word_freq):,}\")\n",
    "print(f\"   Top 10 most common words: {word_freq.most_common(10)}\")\n",
    "\n",
    "# 3. TF-IDF Vectorization\n",
    "print(\"\\n3Ô∏è‚É£ TF-IDF Vectorization:\")\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.95  # Ignore terms that appear in more than 95% of documents\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = tfidf.fit_transform(df_processed['Title_cleaned'])\n",
    "\n",
    "print(f\"   ‚úÖ TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"   ‚úÖ Vocabulary Size: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"   ‚úÖ Feature Names (first 10): {tfidf.get_feature_names_out()[:10]}\")\n",
    "\n",
    "# 4. Most important TF-IDF features\n",
    "print(\"\\n4Ô∏è‚É£ Top TF-IDF Features:\")\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.mean(axis=0).A1\n",
    "feature_importance = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"   Top 20 TF-IDF features:\")\n",
    "for i, (feature, score) in enumerate(feature_importance[:20]):\n",
    "    print(f\"   {i+1:2d}. {feature:20s} ‚Üí {score:.4f}\")\n",
    "\n",
    "# Visualize text statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Title length distribution\n",
    "axes[0, 0].hist(title_lengths, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Title Length Distribution (Characters)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Number of Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. Words per title distribution\n",
    "axes[0, 1].hist(title_words, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Words per Title Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Number of Words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Top 20 words frequency\n",
    "top_words = dict(word_freq.most_common(20))\n",
    "axes[1, 0].bar(range(len(top_words)), list(top_words.values()))\n",
    "axes[1, 0].set_title('Top 20 Most Frequent Words', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Words')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_xticks(range(len(top_words)))\n",
    "axes[1, 0].set_xticklabels(list(top_words.keys()), rotation=45, ha='right')\n",
    "\n",
    "# 4. TF-IDF feature importance\n",
    "top_tfidf = dict(feature_importance[:20])\n",
    "axes[1, 1].bar(range(len(top_tfidf)), [score for _, score in feature_importance[:20]])\n",
    "axes[1, 1].set_title('Top 20 TF-IDF Features', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Features')\n",
    "axes[1, 1].set_ylabel('Average TF-IDF Score')\n",
    "axes[1, 1].set_xticks(range(len(top_tfidf)))\n",
    "axes[1, 1].set_xticklabels([feat for feat, _ in feature_importance[:20]], rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create word cloud\n",
    "print(\"\\n5Ô∏è‚É£ Generating Word Cloud...\")\n",
    "try:\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df_processed['Title_cleaned']))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Publication Titles', fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "    print(\"   ‚úÖ Word cloud generated successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error generating word cloud: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18893ce",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering & Combination\n",
    "\n",
    "### Normalize Numerical Features and Combine with Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering and Combination\n",
    "print(\"‚öôÔ∏è FEATURE ENGINEERING & COMBINATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Normalize numerical features\n",
    "print(\"\\n1Ô∏è‚É£ Normalizing Numerical Features:\")\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_features = df_processed[['Year', 'Cited']].copy()\n",
    "\n",
    "print(f\"   Original Year range: {numerical_features['Year'].min()} - {numerical_features['Year'].max()}\")\n",
    "print(f\"   Original Cited range: {numerical_features['Cited'].min()} - {numerical_features['Cited'].max()}\")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform numerical features\n",
    "numerical_features_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "print(f\"   ‚úÖ Numerical features normalized\")\n",
    "print(f\"   Scaled Year mean: {numerical_features_scaled[:, 0].mean():.4f}, std: {numerical_features_scaled[:, 0].std():.4f}\")\n",
    "print(f\"   Scaled Cited mean: {numerical_features_scaled[:, 1].mean():.4f}, std: {numerical_features_scaled[:, 1].std():.4f}\")\n",
    "\n",
    "# 2. Combine features\n",
    "print(\"\\n2Ô∏è‚É£ Combining Text and Numerical Features:\")\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Combine TF-IDF features with normalized numerical features\n",
    "combined_features = hstack([tfidf_matrix, csr_matrix(numerical_features_scaled)])\n",
    "\n",
    "print(f\"   TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"   Numerical features shape: {numerical_features_scaled.shape}\")\n",
    "print(f\"   ‚úÖ Combined features shape: {combined_features.shape}\")\n",
    "\n",
    "# 3. Create target matrix (multi-label binarization)\n",
    "print(\"\\n3Ô∏è‚É£ Creating Multi-label Target Matrix:\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_multilabel = mlb.fit_transform(df_processed['SDG_labels'])\n",
    "\n",
    "print(f\"   ‚úÖ Target matrix shape: {y_multilabel.shape}\")\n",
    "print(f\"   ‚úÖ Number of unique SDG classes: {len(mlb.classes_)}\")\n",
    "print(f\"   ‚úÖ SDG classes: {mlb.classes_}\")\n",
    "\n",
    "# Display label distribution\n",
    "label_sums = y_multilabel.sum(axis=0)\n",
    "label_df = pd.DataFrame({\n",
    "    'SDG': mlb.classes_,\n",
    "    'Count': label_sums,\n",
    "    'Percentage': (label_sums / len(df_processed)) * 100\n",
    "}).sort_values('Count', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Label Distribution:\")\n",
    "print(label_df)\n",
    "\n",
    "# Visualize feature combination and target distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Numerical features before normalization\n",
    "axes[0, 0].scatter(numerical_features['Year'], numerical_features['Cited'], alpha=0.6)\n",
    "axes[0, 0].set_title('Numerical Features (Original)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Year')\n",
    "axes[0, 0].set_ylabel('Citations')\n",
    "\n",
    "# 2. Numerical features after normalization\n",
    "axes[0, 1].scatter(numerical_features_scaled[:, 0], numerical_features_scaled[:, 1], alpha=0.6)\n",
    "axes[0, 1].set_title('Numerical Features (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Year (Normalized)')\n",
    "axes[0, 1].set_ylabel('Citations (Normalized)')\n",
    "\n",
    "# 3. Target label distribution\n",
    "axes[1, 0].bar(range(len(label_sums)), label_sums)\n",
    "axes[1, 0].set_title('SDG Label Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('SDG Classes')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Labels per document distribution\n",
    "labels_per_doc = y_multilabel.sum(axis=1)\n",
    "axes[1, 1].hist(labels_per_doc, bins=range(1, labels_per_doc.max() + 2), alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Labels per Document Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Number of Labels')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature summary\n",
    "print(f\"\\nüìã FEATURE SUMMARY:\")\n",
    "print(f\"   Text features (TF-IDF): {tfidf_matrix.shape[1]:,}\")\n",
    "print(f\"   Numerical features: {numerical_features_scaled.shape[1]}\")\n",
    "print(f\"   Total combined features: {combined_features.shape[1]:,}\")\n",
    "print(f\"   Total samples: {combined_features.shape[0]:,}\")\n",
    "print(f\"   Target classes: {y_multilabel.shape[1]}\")\n",
    "\n",
    "# Memory usage\n",
    "print(f\"\\nüíæ Memory Usage:\")\n",
    "print(f\"   Combined features matrix: {combined_features.data.nbytes / 1024**2:.1f} MB (sparse)\")\n",
    "print(f\"   Target matrix: {y_multilabel.nbytes / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4e4c1",
   "metadata": {},
   "source": [
    "## 5. Clustering-Based Preprocessing\n",
    "\n",
    "### K-Means and DBSCAN Analysis for Semantic Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Analysis\n",
    "print(\"üîç CLUSTERING ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Find optimal number of clusters using elbow method and silhouette score\n",
    "print(\"\\n1Ô∏è‚É£ Finding Optimal Number of Clusters:\")\n",
    "\n",
    "max_clusters = 20\n",
    "k_range = range(2, max_clusters + 1)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"   Computing K-means for different cluster numbers...\")\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(combined_features)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(combined_features, cluster_labels))\n",
    "    \n",
    "    if k % 5 == 0:\n",
    "        print(f\"   ‚úÖ K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.4f}\")\n",
    "\n",
    "# Find optimal K using silhouette score\n",
    "best_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "best_silhouette_score = max(silhouette_scores)\n",
    "\n",
    "print(f\"\\n   üèÜ Best K (Silhouette): {best_k_silhouette} (Score: {best_silhouette_score:.4f})\")\n",
    "\n",
    "# Plot elbow curve and silhouette scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(k_range, inertias, 'bo-')\n",
    "axes[0].set_title('Elbow Method for Optimal K', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette scores\n",
    "axes[1].plot(k_range, silhouette_scores, 'ro-')\n",
    "axes[1].axvline(x=best_k_silhouette, color='red', linestyle='--', alpha=0.7, label=f'Best K = {best_k_silhouette}')\n",
    "axes[1].set_title('Silhouette Score vs Number of Clusters', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Perform K-means clustering with optimal K\n",
    "print(f\"\\n2Ô∏è‚É£ Performing K-means Clustering (K={best_k_silhouette}):\")\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=best_k_silhouette, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(combined_features)\n",
    "\n",
    "print(f\"   ‚úÖ Clustering completed\")\n",
    "print(f\"   Silhouette Score: {silhouette_score(combined_features, cluster_labels):.4f}\")\n",
    "\n",
    "# Cluster statistics\n",
    "unique_clusters, cluster_counts = np.unique(cluster_labels, return_counts=True)\n",
    "print(f\"   Number of clusters: {len(unique_clusters)}\")\n",
    "print(f\"   Cluster sizes: {dict(zip(unique_clusters, cluster_counts))}\")\n",
    "\n",
    "# 3. Analyze SDG distribution across clusters\n",
    "print(f\"\\n3Ô∏è‚É£ Analyzing SDG Distribution Across Clusters:\")\n",
    "\n",
    "cluster_sdg_analysis = {}\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_mask = cluster_labels == cluster_id\n",
    "    cluster_sdgs = y_multilabel[cluster_mask]\n",
    "    \n",
    "    # Calculate SDG frequency in this cluster\n",
    "    sdg_freq = np.mean(cluster_sdgs, axis=0)\n",
    "    cluster_sdg_analysis[cluster_id] = dict(zip(mlb.classes_, sdg_freq))\n",
    "    \n",
    "    print(f\"   Cluster {cluster_id}: {cluster_counts[cluster_id]} documents\")\n",
    "    # Show top 3 SDGs in this cluster\n",
    "    top_sdgs = sorted(zip(mlb.classes_, sdg_freq), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(f\"      Top SDGs: {', '.join([f'{sdg}({freq:.2f})' for sdg, freq in top_sdgs])}\")\n",
    "\n",
    "# 4. Visualize clustering results\n",
    "print(f\"\\n4Ô∏è‚É£ Visualizing Clustering Results:\")\n",
    "\n",
    "# Use PCA for dimensionality reduction for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "features_2d = pca.fit_transform(combined_features.toarray())\n",
    "\n",
    "print(f\"   PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"   Total explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# Create cluster visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot clusters\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))\n",
    "for i, cluster_id in enumerate(unique_clusters):\n",
    "    mask = cluster_labels == cluster_id\n",
    "    plt.scatter(features_2d[mask, 0], features_2d[mask, 1], \n",
    "               c=[colors[i]], label=f'Cluster {cluster_id} ({cluster_counts[i]})', \n",
    "               alpha=0.6, s=30)\n",
    "\n",
    "plt.title('Document Clustering Visualization (PCA)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel(f'First Principal Component (Explained Variance: {pca.explained_variance_ratio_[0]:.3f})')\n",
    "plt.ylabel(f'Second Principal Component (Explained Variance: {pca.explained_variance_ratio_[1]:.3f})')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Create SDG-cluster heatmap\n",
    "print(f\"\\n5Ô∏è‚É£ Creating SDG-Cluster Distribution Heatmap:\")\n",
    "\n",
    "# Prepare data for heatmap\n",
    "cluster_sdg_matrix = []\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_mask = cluster_labels == cluster_id\n",
    "    cluster_sdgs = y_multilabel[cluster_mask]\n",
    "    sdg_freq = np.mean(cluster_sdgs, axis=0)\n",
    "    cluster_sdg_matrix.append(sdg_freq)\n",
    "\n",
    "cluster_sdg_matrix = np.array(cluster_sdg_matrix)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(cluster_sdg_matrix, \n",
    "           xticklabels=mlb.classes_,\n",
    "           yticklabels=[f'Cluster {c}' for c in unique_clusters],\n",
    "           annot=True, fmt='.2f', cmap='YlOrRd', cbar_kws={'label': 'SDG Frequency'})\n",
    "plt.title('SDG Distribution Across Clusters', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('SDG Categories')\n",
    "plt.ylabel('Clusters')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Add cluster features to the feature matrix\n",
    "print(f\"\\n6Ô∏è‚É£ Adding Cluster Features:\")\n",
    "\n",
    "# Add cluster labels as additional features\n",
    "cluster_features = cluster_labels.reshape(-1, 1)\n",
    "enhanced_features = hstack([combined_features, csr_matrix(cluster_features)])\n",
    "\n",
    "print(f\"   Original features shape: {combined_features.shape}\")\n",
    "print(f\"   Enhanced features shape: {enhanced_features.shape}\")\n",
    "print(f\"   ‚úÖ Cluster features added successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a734d1",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split & Multi-label Preparation\n",
    "\n",
    "### Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9192fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "print(\"üìä TRAIN-TEST SPLIT PREPARATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Prepare features and targets\n",
    "print(\"\\n1Ô∏è‚É£ Preparing Data for Split:\")\n",
    "\n",
    "X = combined_features  # Original features (without cluster)\n",
    "X_enhanced = enhanced_features  # Features with cluster information\n",
    "y = y_multilabel\n",
    "\n",
    "print(f\"   Original features (X): {X.shape}\")\n",
    "print(f\"   Enhanced features (X_enhanced): {X_enhanced.shape}\")\n",
    "print(f\"   Target labels (y): {y.shape}\")\n",
    "\n",
    "# 2. Perform stratified split for multi-label data\n",
    "print(\"\\n2Ô∏è‚É£ Performing Stratified Train-Test Split:\")\n",
    "\n",
    "# For multi-label data, we use a simple split as stratification is complex\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "\n",
    "# Split enhanced features the same way\n",
    "X_enhanced_train, X_enhanced_test, _, _ = train_test_split(\n",
    "    X_enhanced, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "\n",
    "# Further split training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=random_state\n",
    ")\n",
    "\n",
    "X_enhanced_train, X_enhanced_val, _, _ = train_test_split(\n",
    "    X_enhanced_train, y_train, test_size=0.2, random_state=random_state\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Training set: {X_train.shape}\")\n",
    "print(f\"   ‚úÖ Validation set: {X_val.shape}\")\n",
    "print(f\"   ‚úÖ Test set: {X_test.shape}\")\n",
    "\n",
    "# 3. Extract cluster features separately\n",
    "print(\"\\n3Ô∏è‚É£ Extracting Cluster Features:\")\n",
    "\n",
    "# Get cluster features for each split\n",
    "train_indices = np.arange(len(X))[:int(len(X) * 0.8)]  # First 80% for train+val\n",
    "test_indices = np.arange(len(X))[int(len(X) * 0.8):]   # Last 20% for test\n",
    "\n",
    "# Split cluster labels\n",
    "cluster_train_val = cluster_labels[train_indices]\n",
    "cluster_test = cluster_labels[test_indices]\n",
    "\n",
    "# Further split train and validation\n",
    "train_val_split = int(len(cluster_train_val) * 0.8)\n",
    "cluster_train = cluster_train_val[:train_val_split]\n",
    "cluster_val = cluster_train_val[train_val_split:]\n",
    "\n",
    "print(f\"   Cluster features - Train: {cluster_train.shape}\")\n",
    "print(f\"   Cluster features - Validation: {cluster_val.shape}\")\n",
    "print(f\"   Cluster features - Test: {cluster_test.shape}\")\n",
    "\n",
    "# 4. Analyze label distribution across splits\n",
    "print(\"\\n4Ô∏è‚É£ Analyzing Label Distribution Across Splits:\")\n",
    "\n",
    "def analyze_split_distribution(y_split, split_name):\n",
    "    \\\"\\\"\\\"Analyze label distribution in a data split\\\"\\\"\\\"\\n    label_sums = y_split.sum(axis=0)\\n    total_samples = len(y_split)\\n    \\n    print(f\\\"   {split_name}:\\\")\\n    print(f\\\"     Samples: {total_samples:,}\\\")\\n    print(f\\\"     Total labels: {label_sums.sum():,}\\\")\\n    print(f\\\"     Avg labels per sample: {label_sums.sum() / total_samples:.2f}\\\")\\n    print(f\\\"     Most common SDG: {mlb.classes_[np.argmax(label_sums)]} ({label_sums.max()} instances)\\\")\\n    return label_sums\n",
    "\n",
    "train_dist = analyze_split_distribution(y_train, \\\"Train\\\")\\nval_dist = analyze_split_distribution(y_val, \\\"Validation\\\")\\ntest_dist = analyze_split_distribution(y_test, \\\"Test\\\")\n",
    "\n",
    "# 5. Visualize split distributions\n",
    "print(f\\\"\\n5Ô∏è‚É£ Visualizing Split Distributions:\\\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Label distribution comparison\n",
    "split_names = ['Train', 'Validation', 'Test']\n",
    "split_dists = [train_dist, val_dist, test_dist]\n",
    "\n",
    "# Plot 1: Total samples per split\n",
    "sample_counts = [len(y_train), len(y_val), len(y_test)]\n",
    "axes[0, 0].bar(split_names, sample_counts, color=['blue', 'orange', 'green'])\n",
    "axes[0, 0].set_title('Sample Count per Split', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Samples')\n",
    "for i, v in enumerate(sample_counts):\n",
    "    axes[0, 0].text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Total labels per split\n",
    "total_labels = [dist.sum() for dist in split_dists]\n",
    "axes[0, 1].bar(split_names, total_labels, color=['blue', 'orange', 'green'])\n",
    "axes[0, 1].set_title('Total Labels per Split', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Number of Labels')\n",
    "for i, v in enumerate(total_labels):\n",
    "    axes[0, 1].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 3: Average labels per sample\n",
    "avg_labels = [total_labels[i] / sample_counts[i] for i in range(3)]\n",
    "axes[1, 0].bar(split_names, avg_labels, color=['blue', 'orange', 'green'])\n",
    "axes[1, 0].set_title('Average Labels per Sample', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Average Labels')\n",
    "for i, v in enumerate(avg_labels):\n",
    "    axes[1, 0].text(i, v + 0.01, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 4: Label distribution across SDGs (stacked)\n",
    "x_pos = np.arange(len(mlb.classes_))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 1].bar(x_pos - width, train_dist, width, label='Train', alpha=0.8)\n",
    "axes[1, 1].bar(x_pos, val_dist, width, label='Validation', alpha=0.8)\n",
    "axes[1, 1].bar(x_pos + width, test_dist, width, label='Test', alpha=0.8)\n",
    "\n",
    "axes[1, 1].set_title('SDG Label Distribution Across Splits', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('SDG Classes')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(mlb.classes_, rotation=45, ha='right')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Data summary\n",
    "print(f\\\"\\nüìã DATA SPLIT SUMMARY:\\\")\n",
    "print(f\\\"   Original dataset: {len(df_processed):,} samples\\\")\n",
    "print(f\\\"   Training set: {len(y_train):,} samples ({len(y_train)/len(df_processed)*100:.1f}%)\\\")\n",
    "print(f\\\"   Validation set: {len(y_val):,} samples ({len(y_val)/len(df_processed)*100:.1f}%)\\\")\n",
    "print(f\\\"   Test set: {len(y_test):,} samples ({len(y_test)/len(df_processed)*100:.1f}%)\\\")\n",
    "print(f\\\"   Number of features: {X_train.shape[1]:,}\\\")\n",
    "print(f\\\"   Number of enhanced features: {X_enhanced_train.shape[1]:,}\\\")\n",
    "print(f\\\"   Number of target classes: {y_train.shape[1]}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127639f8",
   "metadata": {},
   "source": [
    "## 7. Multi-label Model Training\n",
    "\n",
    "### One-vs-Rest & Hybrid Ensemble Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af56d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-label Model Training\n",
    "print(\"ü§ñ MULTI-LABEL MODEL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Define base models\n",
    "print(\"\\n1Ô∏è‚É£ Defining Base Models:\")\n",
    "\n",
    "base_models = {\n",
    "    'logistic_regression': MultiOutputClassifier(\n",
    "        LogisticRegression(random_state=42, max_iter=1000)\n",
    "    ),\n",
    "    'random_forest': MultiOutputClassifier(\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    )\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    base_models['xgboost'] = MultiOutputClassifier(\n",
    "        xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    )\n",
    "    print(\"   ‚úÖ XGBoost included\")\n",
    "except ImportError:\n",
    "    print(\"   ‚ö†Ô∏è XGBoost not available, skipping\")\n",
    "\n",
    "print(f\"   Available models: {list(base_models.keys())}\")\n",
    "\n",
    "# 2. Train base models\n",
    "print(\"\\n2Ô∏è‚É£ Training Base Models:\")\n",
    "\n",
    "trained_models = {}\n",
    "model_scores = {}\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    print(f\"   Training {name}...\")\n",
    "    \n",
    "    # Train on original features\n",
    "    start_time = pd.Timestamp.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = (pd.Timestamp.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    exact_match = accuracy_score(y_val, y_val_pred)\n",
    "    hamming = hamming_loss(y_val, y_val_pred)\n",
    "    f1_micro = f1_score(y_val, y_val_pred, average='micro')\n",
    "    f1_macro = f1_score(y_val, y_val_pred, average='macro')\n",
    "    jaccard = jaccard_score(y_val, y_val_pred, average='samples')\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    model_scores[name] = {\n",
    "        'exact_match': exact_match,\n",
    "        'hamming_loss': hamming,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'jaccard': jaccard,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"      ‚úÖ Exact Match: {exact_match:.4f}\")\n",
    "    print(f\"      ‚úÖ F1-Micro: {f1_micro:.4f}\")\n",
    "    print(f\"      ‚úÖ Training time: {training_time:.1f}s\")\n",
    "\n",
    "# 3. Train ensemble models\n",
    "print(\"\\n3Ô∏è‚É£ Training Ensemble Models:\")\n",
    "\n",
    "# Voting Classifier\n",
    "print(\"   Training Voting Classifier...\")\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(name, model) for name, model in base_models.items()],\n",
    "    voting='hard'  # Use hard voting for multi-output\n",
    ")\n",
    "\n",
    "start_time = pd.Timestamp.now()\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_time = (pd.Timestamp.now() - start_time).total_seconds()\n",
    "\n",
    "y_val_pred_voting = voting_clf.predict(X_val)\n",
    "\n",
    "# Evaluate voting classifier\n",
    "voting_scores = {\n",
    "    'exact_match': accuracy_score(y_val, y_val_pred_voting),\n",
    "    'hamming_loss': hamming_loss(y_val, y_val_pred_voting),\n",
    "    'f1_micro': f1_score(y_val, y_val_pred_voting, average='micro'),\n",
    "    'f1_macro': f1_score(y_val, y_val_pred_voting, average='macro'),\n",
    "    'jaccard': jaccard_score(y_val, y_val_pred_voting, average='samples'),\n",
    "    'training_time': voting_time\n",
    "}\n",
    "\n",
    "trained_models['voting_classifier'] = voting_clf\n",
    "model_scores['voting_classifier'] = voting_scores\n",
    "\n",
    "print(f\"      ‚úÖ Voting - F1-Micro: {voting_scores['f1_micro']:.4f}\")\n",
    "\n",
    "# 4. Train hybrid models (with cluster features)\n",
    "print(\"\\n4Ô∏è‚É£ Training Hybrid Models (with Cluster Features):\")\n",
    "\n",
    "hybrid_models = {}\n",
    "hybrid_scores = {}\n",
    "\n",
    "for name, base_model in base_models.items():\n",
    "    hybrid_name = f\"hybrid_{name}\"\n",
    "    print(f\"   Training {hybrid_name}...\")\n",
    "    \n",
    "    # Create new instance for hybrid training\n",
    "    if name == 'logistic_regression':\n",
    "        hybrid_model = MultiOutputClassifier(\n",
    "            LogisticRegression(random_state=42, max_iter=1000)\n",
    "        )\n",
    "    elif name == 'random_forest':\n",
    "        hybrid_model = MultiOutputClassifier(\n",
    "            RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        )\n",
    "    elif name == 'xgboost':\n",
    "        hybrid_model = MultiOutputClassifier(\n",
    "            xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "        )\n",
    "    \n",
    "    # Train on enhanced features (with cluster information)\n",
    "    start_time = pd.Timestamp.now()\n",
    "    hybrid_model.fit(X_enhanced_train, y_train)\n",
    "    hybrid_time = (pd.Timestamp.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_val_pred_hybrid = hybrid_model.predict(X_enhanced_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    hybrid_score = {\n",
    "        'exact_match': accuracy_score(y_val, y_val_pred_hybrid),\n",
    "        'hamming_loss': hamming_loss(y_val, y_val_pred_hybrid),\n",
    "        'f1_micro': f1_score(y_val, y_val_pred_hybrid, average='micro'),\n",
    "        'f1_macro': f1_score(y_val, y_val_pred_hybrid, average='macro'),\n",
    "        'jaccard': jaccard_score(y_val, y_val_pred_hybrid, average='samples'),\n",
    "        'training_time': hybrid_time\n",
    "    }\n",
    "    \n",
    "    hybrid_models[hybrid_name] = hybrid_model\n",
    "    hybrid_scores[hybrid_name] = hybrid_score\n",
    "    \n",
    "    print(f\"      ‚úÖ {hybrid_name} - F1-Micro: {hybrid_score['f1_micro']:.4f}\")\n",
    "\n",
    "# Combine all models and scores\n",
    "all_models = {**trained_models, **hybrid_models}\n",
    "all_scores = {**model_scores, **hybrid_scores}\n",
    "\n",
    "# 5. Model comparison\n",
    "print(\"\\n5Ô∏è‚É£ Model Performance Comparison:\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_scores).T\n",
    "comparison_df = comparison_df.round(4)\n",
    "comparison_df = comparison_df.sort_values('f1_micro', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Model Performance Summary (sorted by F1-Micro):\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.index[0]\n",
    "best_score = comparison_df.loc[best_model_name, 'f1_micro']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (F1-Micro: {best_score:.4f})\")\n",
    "\n",
    "# 6. Visualize model comparison\n",
    "print(\"\\n6Ô∏è‚É£ Visualizing Model Performance:\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: F1-Score comparison\n",
    "model_names = list(all_scores.keys())\n",
    "f1_micro_scores = [all_scores[name]['f1_micro'] for name in model_names]\n",
    "f1_macro_scores = [all_scores[name]['f1_macro'] for name in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x_pos - width/2, f1_micro_scores, width, label='F1-Micro', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos + width/2, f1_macro_scores, width, label='F1-Macro', alpha=0.8)\n",
    "axes[0, 0].set_title('F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Models')\n",
    "axes[0, 0].set_ylabel('F1-Score')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Exact Match vs Jaccard\n",
    "exact_match_scores = [all_scores[name]['exact_match'] for name in model_names]\n",
    "jaccard_scores = [all_scores[name]['jaccard'] for name in model_names]\n",
    "\n",
    "axes[0, 1].bar(x_pos - width/2, exact_match_scores, width, label='Exact Match', alpha=0.8)\n",
    "axes[0, 1].bar(x_pos + width/2, jaccard_scores, width, label='Jaccard', alpha=0.8)\n",
    "axes[0, 1].set_title('Exact Match vs Jaccard Similarity', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Models')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Hamming Loss\n",
    "hamming_scores = [all_scores[name]['hamming_loss'] for name in model_names]\n",
    "\n",
    "axes[1, 0].bar(model_names, hamming_scores, alpha=0.8, color='red')\n",
    "axes[1, 0].set_title('Hamming Loss (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Models')\n",
    "axes[1, 0].set_ylabel('Hamming Loss')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Training Time\n",
    "training_times = [all_scores[name]['training_time'] for name in model_names]\n",
    "\n",
    "axes[1, 1].bar(model_names, training_times, alpha=0.8, color='green')\n",
    "axes[1, 1].set_title('Training Time (seconds)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Models')\n",
    "axes[1, 1].set_ylabel('Time (seconds)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Model training completed!\")\n",
    "print(f\"   Total models trained: {len(all_models)}\")\n",
    "print(f\"   Best performing model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ba959",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation & Visualization\n",
    "\n",
    "### Comprehensive Multi-label Metrics and Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Evaluation\n",
    "print(\"üìà COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Evaluate best model on test set\n",
    "print(f\"\\n1Ô∏è‚É£ Final Evaluation of Best Model ({best_model_name}):\")\n",
    "\n",
    "best_model = all_models[best_model_name]\n",
    "\n",
    "# Determine if it's a hybrid model\n",
    "if best_model_name.startswith('hybrid_'):\n",
    "    y_test_pred = best_model.predict(X_enhanced_test)\n",
    "    X_test_eval = X_enhanced_test\n",
    "else:\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    X_test_eval = X_test\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "test_metrics = {\n",
    "    'exact_match_ratio': accuracy_score(y_test, y_test_pred),\n",
    "    'hamming_loss': hamming_loss(y_test, y_test_pred),\n",
    "    'jaccard_similarity': jaccard_score(y_test, y_test_pred, average='samples'),\n",
    "    'precision_micro': precision_score(y_test, y_test_pred, average='micro'),\n",
    "    'precision_macro': precision_score(y_test, y_test_pred, average='macro'),\n",
    "    'precision_weighted': precision_score(y_test, y_test_pred, average='weighted'),\n",
    "    'recall_micro': recall_score(y_test, y_test_pred, average='micro'),\n",
    "    'recall_macro': recall_score(y_test, y_test_pred, average='macro'),\n",
    "    'recall_weighted': recall_score(y_test, y_test_pred, average='weighted'),\n",
    "    'f1_micro': f1_score(y_test, y_test_pred, average='micro'),\n",
    "    'f1_macro': f1_score(y_test, y_test_pred, average='macro'),\n",
    "    'f1_weighted': f1_score(y_test, y_test_pred, average='weighted')\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Final Test Set Results:\")\n",
    "print(f\"   Exact Match Ratio: {test_metrics['exact_match_ratio']:.4f}\")\n",
    "print(f\"   Hamming Loss: {test_metrics['hamming_loss']:.4f}\")\n",
    "print(f\"   Jaccard Similarity: {test_metrics['jaccard_similarity']:.4f}\")\n",
    "print(f\"\\n   Precision (Micro/Macro/Weighted): {test_metrics['precision_micro']:.4f} / {test_metrics['precision_macro']:.4f} / {test_metrics['precision_weighted']:.4f}\")\n",
    "print(f\"   Recall (Micro/Macro/Weighted): {test_metrics['recall_micro']:.4f} / {test_metrics['recall_macro']:.4f} / {test_metrics['recall_weighted']:.4f}\")\n",
    "print(f\"   F1-Score (Micro/Macro/Weighted): {test_metrics['f1_micro']:.4f} / {test_metrics['f1_macro']:.4f} / {test_metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "# 2. Per-class performance analysis\n",
    "print(f\"\\n2Ô∏è‚É£ Per-Class Performance Analysis:\")\n",
    "\n",
    "# Calculate per-class metrics\n",
    "precision_per_class = precision_score(y_test, y_test_pred, average=None)\n",
    "recall_per_class = recall_score(y_test, y_test_pred, average=None)\n",
    "f1_per_class = f1_score(y_test, y_test_pred, average=None)\n",
    "\n",
    "# Create per-class results DataFrame\n",
    "per_class_df = pd.DataFrame({\n",
    "    'SDG': mlb.classes_,\n",
    "    'Precision': precision_per_class,\n",
    "    'Recall': recall_per_class,\n",
    "    'F1-Score': f1_per_class,\n",
    "    'Support': y_test.sum(axis=0)\n",
    "}).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Per-Class Performance (Top 10):\")\n",
    "print(per_class_df.head(10).round(4))\n",
    "\n",
    "# 3. Confusion Matrix Analysis\n",
    "print(f\"\\n3Ô∏è‚É£ Multi-label Confusion Matrix Analysis:\")\n",
    "\n",
    "# Calculate confusion matrix for each class\n",
    "cm_array = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Display confusion matrix for top 3 performing classes\n",
    "top_3_classes = per_class_df.head(3)['SDG'].values\n",
    "top_3_indices = [list(mlb.classes_).index(sdg) for sdg in top_3_classes]\n",
    "\n",
    "print(f\"   Showing confusion matrices for top 3 performing classes:\")\n",
    "for i, (sdg, idx) in enumerate(zip(top_3_classes, top_3_indices)):\n",
    "    cm = cm_array[idx]\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\n   {sdg}:\")\n",
    "    print(f\"      True Negatives: {tn}, False Positives: {fp}\")\n",
    "    print(f\"      False Negatives: {fn}, True Positives: {tp}\")\n",
    "    print(f\"      Precision: {tp/(tp+fp):.4f}, Recall: {tp/(tp+fn):.4f}\")\n",
    "\n",
    "# 4. Visualizations\n",
    "print(f\"\\n4Ô∏è‚É£ Creating Detailed Visualizations:\")\n",
    "\n",
    "# Visualization 1: Per-class performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Plot 1: Per-class F1-scores\n",
    "axes[0, 0].bar(range(len(per_class_df)), per_class_df['F1-Score'], alpha=0.8)\n",
    "axes[0, 0].set_title('Per-Class F1-Score Performance', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('SDG Classes')\n",
    "axes[0, 0].set_ylabel('F1-Score')\n",
    "axes[0, 0].set_xticks(range(len(per_class_df)))\n",
    "axes[0, 0].set_xticklabels(per_class_df['SDG'], rotation=45, ha='right')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Precision vs Recall scatter\n",
    "axes[0, 1].scatter(per_class_df['Precision'], per_class_df['Recall'], \n",
    "                  s=per_class_df['Support']*2, alpha=0.6)\n",
    "axes[0, 1].set_title('Precision vs Recall (bubble size = support)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Precision')\n",
    "axes[0, 1].set_ylabel('Recall')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add labels for each point\n",
    "for i, row in per_class_df.iterrows():\n",
    "    axes[0, 1].annotate(row['SDG'], (row['Precision'], row['Recall']), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Plot 3: Metrics comparison\n",
    "metrics_names = ['Precision', 'Recall', 'F1-Score']\n",
    "micro_scores = [test_metrics['precision_micro'], test_metrics['recall_micro'], test_metrics['f1_micro']]\n",
    "macro_scores = [test_metrics['precision_macro'], test_metrics['recall_macro'], test_metrics['f1_macro']]\n",
    "weighted_scores = [test_metrics['precision_weighted'], test_metrics['recall_weighted'], test_metrics['f1_weighted']]\n",
    "\n",
    "x_pos = np.arange(len(metrics_names))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 0].bar(x_pos - width, micro_scores, width, label='Micro', alpha=0.8)\n",
    "axes[1, 0].bar(x_pos, macro_scores, width, label='Macro', alpha=0.8)\n",
    "axes[1, 0].bar(x_pos + width, weighted_scores, width, label='Weighted', alpha=0.8)\n",
    "\n",
    "axes[1, 0].set_title('Averaging Methods Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Metrics')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(metrics_names)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Model comparison (validation vs test)\n",
    "if len(all_scores) > 1:\n",
    "    # Get top 5 models for comparison\n",
    "    top_models = list(comparison_df.head(5).index)\n",
    "    \n",
    "    val_f1_scores = [all_scores[model]['f1_micro'] for model in top_models]\n",
    "    \n",
    "    # Calculate test scores for top models\n",
    "    test_f1_scores = []\n",
    "    for model_name in top_models:\n",
    "        model = all_models[model_name]\n",
    "        if model_name.startswith('hybrid_'):\n",
    "            y_pred_temp = model.predict(X_enhanced_test)\n",
    "        else:\n",
    "            y_pred_temp = model.predict(X_test)\n",
    "        test_f1_scores.append(f1_score(y_test, y_pred_temp, average='micro'))\n",
    "    \n",
    "    x_pos = np.arange(len(top_models))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 1].bar(x_pos - width/2, val_f1_scores, width, label='Validation', alpha=0.8)\n",
    "    axes[1, 1].bar(x_pos + width/2, test_f1_scores, width, label='Test', alpha=0.8)\n",
    "    axes[1, 1].set_title('Validation vs Test Performance', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Models')\n",
    "    axes[1, 1].set_ylabel('F1-Score (Micro)')\n",
    "    axes[1, 1].set_xticks(x_pos)\n",
    "    axes[1, 1].set_xticklabels(top_models, rotation=45, ha='right')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Confusion Matrix Heatmap for top classes\n",
    "print(f\"\\n5Ô∏è‚É£ Confusion Matrix Heatmaps:\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (sdg, idx) in enumerate(zip(top_3_classes, top_3_indices)):\n",
    "    cm = cm_array[idx]\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "    axes[i].set_title(f'Confusion Matrix - {sdg}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Error Analysis\n",
    "print(f\"\\n6Ô∏è‚É£ Error Analysis:\")\n",
    "\n",
    "# Calculate prediction errors\n",
    "prediction_errors = np.abs(y_test - y_test_pred)\n",
    "error_per_sample = prediction_errors.sum(axis=1)\n",
    "error_per_class = prediction_errors.sum(axis=0)\n",
    "\n",
    "print(f\"   Total prediction errors: {prediction_errors.sum():,}\")\n",
    "print(f\"   Average errors per sample: {error_per_sample.mean():.2f}\")\n",
    "print(f\"   Samples with no errors: {sum(error_per_sample == 0):,} ({sum(error_per_sample == 0)/len(error_per_sample)*100:.1f}%)\")\n",
    "print(f\"   Samples with 1 error: {sum(error_per_sample == 1):,} ({sum(error_per_sample == 1)/len(error_per_sample)*100:.1f}%)\")\n",
    "print(f\"   Samples with 2+ errors: {sum(error_per_sample >= 2):,} ({sum(error_per_sample >= 2)/len(error_per_sample)*100:.1f}%)\")\n",
    "\n",
    "# Most problematic classes\n",
    "error_class_df = pd.DataFrame({\n",
    "    'SDG': mlb.classes_,\n",
    "    'Total_Errors': error_per_class,\n",
    "    'Error_Rate': error_per_class / y_test.sum(axis=0)\n",
    "}).sort_values('Total_Errors', ascending=False)\n",
    "\n",
    "print(f\"\\n   Most problematic classes (by total errors):\")\n",
    "print(error_class_df.head().round(4))\n",
    "\n",
    "print(f\"\\n‚úÖ Comprehensive evaluation completed!\")\n",
    "print(f\"   Best model: {best_model_name}\")\n",
    "print(f\"   Final F1-Score (Micro): {test_metrics['f1_micro']:.4f}\")\n",
    "print(f\"   Final F1-Score (Macro): {test_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   Exact Match Ratio: {test_metrics['exact_match_ratio']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7a6fd",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning & Cross-Validation\n",
    "\n",
    "### GridSearchCV and Stratified K-Fold for Multi-label Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e92dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning and Cross-Validation\n",
    "print(\"üîß HYPERPARAMETER TUNING & CROSS-VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Define parameter grids for tuning\n",
    "print(\"\\n1Ô∏è‚É£ Defining Parameter Grids:\")\n",
    "\n",
    "param_grids = {\n",
    "    'logistic_regression': {\n",
    "        'estimator__C': [0.1, 1.0, 10.0],\n",
    "        'estimator__penalty': ['l1', 'l2'],\n",
    "        'estimator__solver': ['liblinear']\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'estimator__n_estimators': [50, 100, 200],\n",
    "        'estimator__max_depth': [10, 20, None],\n",
    "        'estimator__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add XGBoost parameters if available\n",
    "if 'xgboost' in base_models:\n",
    "    param_grids['xgboost'] = {\n",
    "        'estimator__n_estimators': [50, 100, 200],\n",
    "        'estimator__max_depth': [3, 6, 10],\n",
    "        'estimator__learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "print(f\"   Parameter grids defined for: {list(param_grids.keys())}\")\n",
    "\n",
    "# 2. Cross-validation setup\n",
    "print(\"\\n2Ô∏è‚É£ Setting up Cross-Validation:\")\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define scoring metric\n",
    "scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "# Combine training and validation sets for cross-validation\n",
    "X_train_full = np.vstack([X_train.toarray(), X_val.toarray()])\n",
    "X_enhanced_train_full = np.vstack([X_enhanced_train.toarray(), X_enhanced_val.toarray()])\n",
    "y_train_full = np.vstack([y_train, y_val])\n",
    "\n",
    "print(f\"   Full training set shape: {X_train_full.shape}\")\n",
    "print(f\"   Enhanced training set shape: {X_enhanced_train_full.shape}\")\n",
    "print(f\"   Scoring metric: F1-Score (Micro)\")\n",
    "\n",
    "# 3. Hyperparameter tuning for selected models\n",
    "print(\"\\n3Ô∏è‚É£ Performing Hyperparameter Tuning:\")\n",
    "\n",
    "tuned_models = {}\n",
    "tuning_results = {}\n",
    "\n",
    "# Tune top 2 models to save computation time\n",
    "models_to_tune = ['logistic_regression', 'random_forest']\n",
    "\n",
    "for model_name in models_to_tune:\n",
    "    if model_name in param_grids:\n",
    "        print(f\"\\n   Tuning {model_name}...\")\n",
    "        \n",
    "        # Create fresh model instance\n",
    "        if model_name == 'logistic_regression':\n",
    "            base_model = MultiOutputClassifier(\n",
    "                LogisticRegression(random_state=42, max_iter=1000)\n",
    "            )\n",
    "        elif model_name == 'random_forest':\n",
    "            base_model = MultiOutputClassifier(\n",
    "                RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "            )\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            base_model, \n",
    "            param_grids[model_name], \n",
    "            cv=3,  # 3-fold CV to save time\n",
    "            scoring=scorer, \n",
    "            n_jobs=-1, \n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit grid search\n",
    "        start_time = pd.Timestamp.now()\n",
    "        grid_search.fit(X_train_full, y_train_full)\n",
    "        tuning_time = (pd.Timestamp.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Store results\n",
    "        tuned_models[f\"tuned_{model_name}\"] = grid_search.best_estimator_\n",
    "        tuning_results[model_name] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'tuning_time': tuning_time\n",
    "        }\n",
    "        \n",
    "        print(f\"      ‚úÖ Best score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"      ‚úÖ Best params: {grid_search.best_params_}\")\n",
    "        print(f\"      ‚úÖ Tuning time: {tuning_time:.1f}s\")\n",
    "\n",
    "# 4. Cross-validation analysis\n",
    "print(\"\\n4Ô∏è‚É£ Cross-Validation Analysis:\")\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "# Perform cross-validation on the best models\n",
    "top_models_for_cv = ['logistic_regression', 'random_forest']\n",
    "if 'xgboost' in base_models:\n",
    "    top_models_for_cv.append('xgboost')\n",
    "\n",
    "for model_name in top_models_for_cv:\n",
    "    print(f\"\\n   Cross-validating {model_name}...\")\n",
    "    \n",
    "    # Use tuned model if available, otherwise use base model\n",
    "    if f\"tuned_{model_name}\" in tuned_models:\n",
    "        model = tuned_models[f\"tuned_{model_name}\"]\n",
    "        print(f\"      Using tuned parameters\")\n",
    "    else:\n",
    "        model = base_models[model_name]\n",
    "        print(f\"      Using default parameters\")\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_full, y_train_full, \n",
    "                               cv=5, scoring=scorer, n_jobs=-1)\n",
    "    \n",
    "    cv_results[model_name] = {\n",
    "        'scores': cv_scores,\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"      CV Scores: {cv_scores}\")\n",
    "    print(f\"      Mean ¬± Std: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "# 5. Hybrid model tuning\n",
    "print(\"\\n5Ô∏è‚É£ Hybrid Model Optimization:\")\n",
    "\n",
    "# Test if adding cluster features improves the best model\n",
    "best_base_model = max(cv_results.items(), key=lambda x: x[1]['mean'])\n",
    "best_base_name = best_base_model[0]\n",
    "\n",
    "print(f\"   Best base model: {best_base_name}\")\n",
    "print(f\"   Testing hybrid version with cluster features...\")\n",
    "\n",
    "# Create hybrid version of best model\n",
    "if best_base_name == 'logistic_regression':\n",
    "    hybrid_model = MultiOutputClassifier(\n",
    "        LogisticRegression(random_state=42, max_iter=1000)\n",
    "    )\n",
    "elif best_base_name == 'random_forest':\n",
    "    hybrid_model = MultiOutputClassifier(\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    )\n",
    "\n",
    "# Cross-validate hybrid model\n",
    "hybrid_cv_scores = cross_val_score(hybrid_model, X_enhanced_train_full, y_train_full, \n",
    "                                  cv=5, scoring=scorer, n_jobs=-1)\n",
    "\n",
    "print(f\"   Hybrid CV Scores: {hybrid_cv_scores}\")\n",
    "print(f\"   Hybrid Mean ¬± Std: {hybrid_cv_scores.mean():.4f} ¬± {hybrid_cv_scores.std():.4f}\")\n",
    "\n",
    "# Compare base vs hybrid\n",
    "improvement = hybrid_cv_scores.mean() - cv_results[best_base_name]['mean']\n",
    "print(f\"   Improvement with clustering: {improvement:.4f}\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"   ‚úÖ Hybrid approach shows improvement!\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Hybrid approach shows minimal improvement\")\n",
    "\n",
    "# 6. Results visualization\n",
    "print(\"\\n6Ô∏è‚É£ Visualizing Tuning Results:\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Cross-validation scores comparison\n",
    "model_names = list(cv_results.keys())\n",
    "cv_means = [cv_results[name]['mean'] for name in model_names]\n",
    "cv_stds = [cv_results[name]['std'] for name in model_names]\n",
    "\n",
    "axes[0, 0].bar(model_names, cv_means, yerr=cv_stds, capsize=5, alpha=0.8)\n",
    "axes[0, 0].set_title('Cross-Validation Scores Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Models')\n",
    "axes[0, 0].set_ylabel('F1-Score (Mean ¬± Std)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Base vs Hybrid comparison\n",
    "if improvement is not None:\n",
    "    comparison_models = [best_base_name, f'hybrid_{best_base_name}']\n",
    "    comparison_scores = [cv_results[best_base_name]['mean'], hybrid_cv_scores.mean()]\n",
    "    comparison_stds = [cv_results[best_base_name]['std'], hybrid_cv_scores.std()]\n",
    "    \n",
    "    axes[0, 1].bar(comparison_models, comparison_scores, yerr=comparison_stds, \n",
    "                   capsize=5, alpha=0.8, color=['blue', 'orange'])\n",
    "    axes[0, 1].set_title('Base vs Hybrid Model Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Model Type')\n",
    "    axes[0, 1].set_ylabel('F1-Score (Mean ¬± Std)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Hyperparameter tuning results\n",
    "if tuning_results:\n",
    "    tuned_model_names = list(tuning_results.keys())\n",
    "    tuning_scores = [tuning_results[name]['best_score'] for name in tuned_model_names]\n",
    "    base_scores = [cv_results[name]['mean'] for name in tuned_model_names]\n",
    "    \n",
    "    x_pos = np.arange(len(tuned_model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].bar(x_pos - width/2, base_scores, width, label='Default', alpha=0.8)\n",
    "    axes[1, 0].bar(x_pos + width/2, tuning_scores, width, label='Tuned', alpha=0.8)\n",
    "    axes[1, 0].set_title('Default vs Tuned Parameters', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Models')\n",
    "    axes[1, 0].set_ylabel('F1-Score')\n",
    "    axes[1, 0].set_xticks(x_pos)\n",
    "    axes[1, 0].set_xticklabels(tuned_model_names)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: CV score distributions\n",
    "if cv_results:\n",
    "    # Create box plot of CV scores\n",
    "    cv_scores_list = [cv_results[name]['scores'] for name in model_names]\n",
    "    \n",
    "    axes[1, 1].boxplot(cv_scores_list, labels=model_names)\n",
    "    axes[1, 1].set_title('Cross-Validation Score Distributions', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Models')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Final model selection\n",
    "print(\"\\n7Ô∏è‚É£ Final Model Selection:\")\n",
    "\n",
    "final_model_candidates = {}\n",
    "\n",
    "# Add cross-validated models\n",
    "for name, results in cv_results.items():\n",
    "    final_model_candidates[name] = results['mean']\n",
    "\n",
    "# Add hybrid model\n",
    "if improvement is not None:\n",
    "    final_model_candidates[f'hybrid_{best_base_name}'] = hybrid_cv_scores.mean()\n",
    "\n",
    "# Add tuned models\n",
    "for name, results in tuning_results.items():\n",
    "    final_model_candidates[f'tuned_{name}'] = results['best_score']\n",
    "\n",
    "# Find the best final model\n",
    "final_best_model = max(final_model_candidates.items(), key=lambda x: x[1])\n",
    "final_best_name, final_best_score = final_best_model\n",
    "\n",
    "print(f\"\\nüèÜ FINAL MODEL SELECTION:\")\n",
    "print(f\"   Best Model: {final_best_name}\")\n",
    "print(f\"   Best Score: {final_best_score:.4f}\")\n",
    "\n",
    "# Display all candidates\n",
    "print(f\"\\nüìä All Final Candidates:\")\n",
    "sorted_candidates = sorted(final_model_candidates.items(), key=lambda x: x[1], reverse=True)\n",
    "for name, score in sorted_candidates:\n",
    "    print(f\"   {name:25s}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Hyperparameter tuning and cross-validation completed!\")\n",
    "print(f\"   Best approach: {final_best_name}\")\n",
    "print(f\"   Expected performance: {final_best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f404d57c",
   "metadata": {},
   "source": [
    "## 10. Model Saving & Inference Example\n",
    "\n",
    "### Save Models, Vectorizer, and Pipeline for Production Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35fb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving and Inference Pipeline\n",
    "print(\"üíæ MODEL SAVING & INFERENCE PIPELINE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Prepare models for saving\n",
    "print(\"\\n1Ô∏è‚É£ Preparing Models for Saving:\")\n",
    "\n",
    "# Create models directory\n",
    "import os\n",
    "models_dir = \"../models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Determine the final best model to save\n",
    "if final_best_name.startswith('tuned_'):\n",
    "    model_to_save = tuned_models[final_best_name]\n",
    "    print(f\"   Selected tuned model: {final_best_name}\")\n",
    "elif final_best_name.startswith('hybrid_'):\n",
    "    # Train the hybrid model one more time on full training data\n",
    "    base_name = final_best_name.replace('hybrid_', '')\n",
    "    if base_name == 'logistic_regression':\n",
    "        model_to_save = MultiOutputClassifier(\n",
    "            LogisticRegression(random_state=42, max_iter=1000)\n",
    "        )\n",
    "    elif base_name == 'random_forest':\n",
    "        model_to_save = MultiOutputClassifier(\n",
    "            RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        )\n",
    "    \n",
    "    model_to_save.fit(X_enhanced_train_full, y_train_full)\n",
    "    print(f\"   Selected hybrid model: {final_best_name}\")\n",
    "else:\n",
    "    model_to_save = all_models[final_best_name]\n",
    "    print(f\"   Selected base model: {final_best_name}\")\n",
    "\n",
    "print(f\"   Model type: {type(model_to_save)}\")\n",
    "\n",
    "# 2. Save all components\n",
    "print(\"\\n2Ô∏è‚É£ Saving Model Components:\")\n",
    "\n",
    "try:\n",
    "    # Save the main model\n",
    "    model_path = os.path.join(models_dir, \"best_model.joblib\")\n",
    "    joblib.dump(model_to_save, model_path)\n",
    "    print(f\"   ‚úÖ Model saved: {model_path}\")\n",
    "    \n",
    "    # Save the TF-IDF vectorizer\n",
    "    vectorizer_path = os.path.join(models_dir, \"tfidf_vectorizer.joblib\")\n",
    "    joblib.dump(tfidf, vectorizer_path)\n",
    "    print(f\"   ‚úÖ TF-IDF vectorizer saved: {vectorizer_path}\")\n",
    "    \n",
    "    # Save the feature scaler\n",
    "    scaler_path = os.path.join(models_dir, \"feature_scaler.joblib\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"   ‚úÖ Feature scaler saved: {scaler_path}\")\n",
    "    \n",
    "    # Save the multi-label binarizer\n",
    "    mlb_path = os.path.join(models_dir, \"multilabel_binarizer.joblib\")\n",
    "    joblib.dump(mlb, mlb_path)\n",
    "    print(f\"   ‚úÖ Multi-label binarizer saved: {mlb_path}\")\n",
    "    \n",
    "    # Save the K-means clustering model\n",
    "    kmeans_path = os.path.join(models_dir, \"kmeans_model.joblib\")\n",
    "    joblib.dump(kmeans_final, kmeans_path)\n",
    "    print(f\"   ‚úÖ K-means model saved: {kmeans_path}\")\n",
    "    \n",
    "    # Save model metadata\n",
    "    metadata = {\n",
    "        'model_name': final_best_name,\n",
    "        'model_score': final_best_score,\n",
    "        'feature_names': tfidf.get_feature_names_out().tolist() + ['Year', 'Cited'],\n",
    "        'target_names': mlb.classes_.tolist(),\n",
    "        'n_clusters': best_k_silhouette,\n",
    "        'is_hybrid': final_best_name.startswith('hybrid_'),\n",
    "        'test_metrics': test_metrics\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(models_dir, \"model_metadata.joblib\")\n",
    "    joblib.dump(metadata, metadata_path)\n",
    "    print(f\"   ‚úÖ Model metadata saved: {metadata_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error saving models: {e}\")\n",
    "\n",
    "# 3. Create inference pipeline\n",
    "print(\"\\n3Ô∏è‚É£ Creating Inference Pipeline:\")\n",
    "\n",
    "class SDGPredictor:\n",
    "    \\\"\\\"\\\"Production-ready SDG prediction pipeline\\\"\\\"\\\"\n",
    "    \n",
    "    def __init__(self, models_dir):\n",
    "        self.models_dir = models_dir\n",
    "        self.model = None\n",
    "        self.tfidf = None\n",
    "        self.scaler = None\n",
    "        self.mlb = None\n",
    "        self.kmeans = None\n",
    "        self.metadata = None\n",
    "        self.is_loaded = False\n",
    "        \n",
    "    def load_models(self):\n",
    "        \\\"\\\"\\\"Load all saved models and components\\\"\\\"\\\"\n",
    "        try:\n",
    "            self.model = joblib.load(os.path.join(self.models_dir, \"best_model.joblib\"))\n",
    "            self.tfidf = joblib.load(os.path.join(self.models_dir, \"tfidf_vectorizer.joblib\"))\n",
    "            self.scaler = joblib.load(os.path.join(self.models_dir, \"feature_scaler.joblib\"))\n",
    "            self.mlb = joblib.load(os.path.join(self.models_dir, \"multilabel_binarizer.joblib\"))\n",
    "            self.kmeans = joblib.load(os.path.join(self.models_dir, \"kmeans_model.joblib\"))\n",
    "            self.metadata = joblib.load(os.path.join(self.models_dir, \"model_metadata.joblib\"))\n",
    "            \n",
    "            self.is_loaded = True\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \\\"\\\"\\\"Clean and preprocess text\\\"\\\"\\\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\\\s]', '', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def predict(self, title, year=None, cited=None):\n",
    "        \\\"\\\"\\\"Predict SDG labels for a single publication\\\"\\\"\\\"\n",
    "        if not self.is_loaded:\n",
    "            raise ValueError(\"Models not loaded. Call load_models() first.\")\n",
    "        \n",
    "        # Preprocess inputs\n",
    "        title_clean = self.preprocess_text(title)\n",
    "        year = year if year else 2020\n",
    "        cited = cited if cited else 0\n",
    "        \n",
    "        # Create feature vector\n",
    "        tfidf_features = self.tfidf.transform([title_clean])\n",
    "        numerical_features = self.scaler.transform([[year, cited]])\n",
    "        \n",
    "        # Combine features\n",
    "        from scipy.sparse import hstack, csr_matrix\n",
    "        combined_features = hstack([tfidf_features, csr_matrix(numerical_features)])\n",
    "        \n",
    "        # Add cluster feature if hybrid model\n",
    "        if self.metadata['is_hybrid']:\n",
    "            cluster_label = self.kmeans.predict(combined_features)[0]\n",
    "            enhanced_features = hstack([combined_features, csr_matrix([[cluster_label]])])\n",
    "            features_final = enhanced_features\n",
    "        else:\n",
    "            features_final = combined_features\n",
    "        \n",
    "        # Make prediction\n",
    "        y_pred = self.model.predict(features_final)\n",
    "        y_proba = None\n",
    "        \n",
    "        # Get probabilities if available\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            try:\n",
    "                y_proba = self.model.predict_proba(features_final)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Convert to readable format\n",
    "        predicted_sdgs = []\n",
    "        probabilities = {}\n",
    "        \n",
    "        for i, class_name in enumerate(self.metadata['target_names']):\n",
    "            if y_pred[0][i] == 1:\n",
    "                predicted_sdgs.append(class_name)\n",
    "            \n",
    "            if y_proba is not None:\n",
    "                if isinstance(y_proba, list):\n",
    "                    probabilities[class_name] = float(y_proba[i][0][1])\n",
    "                else:\n",
    "                    probabilities[class_name] = float(y_proba[0][i])\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'predicted_sdgs': predicted_sdgs,\n",
    "            'probabilities': probabilities,\n",
    "            'model_used': self.metadata['model_name']\n",
    "        }\n",
    "\n",
    "# Initialize and test the predictor\n",
    "predictor = SDGPredictor(models_dir)\n",
    "if predictor.load_models():\n",
    "    print(f\"   ‚úÖ Inference pipeline loaded successfully!\")\n",
    "    print(f\"   Model: {predictor.metadata['model_name']}\")\n",
    "    print(f\"   Expected score: {predictor.metadata['model_score']:.4f}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Failed to load inference pipeline\")\n",
    "\n",
    "# 4. Test inference with examples\n",
    "print(\"\\n4Ô∏è‚É£ Testing Inference Pipeline:\")\n",
    "\n",
    "test_examples = [\n",
    "    {\n",
    "        'title': 'Machine learning approaches for sustainable energy management',\n",
    "        'year': 2023,\n",
    "        'cited': 15\n",
    "    },\n",
    "    {\n",
    "        'title': 'Climate change impacts on biodiversity conservation',\n",
    "        'year': 2022,\n",
    "        'cited': 45\n",
    "    },\n",
    "    {\n",
    "        'title': 'Healthcare innovation using artificial intelligence',\n",
    "        'year': 2023,\n",
    "        'cited': 28\n",
    "    },\n",
    "    {\n",
    "        'title': 'Educational technology for inclusive learning environments',\n",
    "        'year': 2021,\n",
    "        'cited': 12\n",
    "    }\n",
    "]\n",
    "\n",
    "if predictor.is_loaded:\n",
    "    print(f\"\\\\n   Testing with {len(test_examples)} sample publications:\")\n",
    "    \n",
    "    for i, example in enumerate(test_examples):\n",
    "        try:\n",
    "            result = predictor.predict(\n",
    "                title=example['title'],\n",
    "                year=example['year'],\n",
    "                cited=example['cited']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\\\n   Example {i+1}:\")\n",
    "            print(f\"      Title: {example['title'][:60]}...\")\n",
    "            print(f\"      Predicted SDGs: {', '.join(result['predicted_sdgs']) if result['predicted_sdgs'] else 'None'}\")\n",
    "            \n",
    "            if result['probabilities']:\n",
    "                top_probs = sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "                print(f\"      Top probabilities: {', '.join([f'{sdg}({prob:.3f})' for sdg, prob in top_probs])}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error in example {i+1}: {e}\")\n",
    "\n",
    "# 5. Create standalone prediction script\n",
    "print(\"\\\\n5Ô∏è‚É£ Creating Standalone Prediction Script:\")\n",
    "\n",
    "predict_script = '''#!/usr/bin/env python3\n",
    "\\\"\\\"\\\"\n",
    "Standalone SDG prediction script\n",
    "Usage: python predict_sample.py \"publication title\" [year] [cited]\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "class SDGPredictor:\n",
    "    def __init__(self, models_dir=\"models\"):\n",
    "        self.models_dir = models_dir\n",
    "        self.model = None\n",
    "        self.tfidf = None\n",
    "        self.scaler = None\n",
    "        self.mlb = None\n",
    "        self.kmeans = None\n",
    "        self.metadata = None\n",
    "        self.is_loaded = False\n",
    "        \n",
    "    def load_models(self):\n",
    "        try:\n",
    "            self.model = joblib.load(os.path.join(self.models_dir, \"best_model.joblib\"))\n",
    "            self.tfidf = joblib.load(os.path.join(self.models_dir, \"tfidf_vectorizer.joblib\"))\n",
    "            self.scaler = joblib.load(os.path.join(self.models_dir, \"feature_scaler.joblib\"))\n",
    "            self.mlb = joblib.load(os.path.join(self.models_dir, \"multilabel_binarizer.joblib\"))\n",
    "            self.kmeans = joblib.load(os.path.join(self.models_dir, \"kmeans_model.joblib\"))\n",
    "            self.metadata = joblib.load(os.path.join(self.models_dir, \"model_metadata.joblib\"))\n",
    "            self.is_loaded = True\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\\\\\\\s]', '', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def predict(self, title, year=None, cited=None):\n",
    "        if not self.is_loaded:\n",
    "            raise ValueError(\"Models not loaded\")\n",
    "        \n",
    "        title_clean = self.preprocess_text(title)\n",
    "        year = year if year else 2020\n",
    "        cited = cited if cited else 0\n",
    "        \n",
    "        tfidf_features = self.tfidf.transform([title_clean])\n",
    "        numerical_features = self.scaler.transform([[year, cited]])\n",
    "        combined_features = hstack([tfidf_features, csr_matrix(numerical_features)])\n",
    "        \n",
    "        if self.metadata['is_hybrid']:\n",
    "            cluster_label = self.kmeans.predict(combined_features)[0]\n",
    "            features_final = hstack([combined_features, csr_matrix([[cluster_label]])])\n",
    "        else:\n",
    "            features_final = combined_features\n",
    "        \n",
    "        y_pred = self.model.predict(features_final)\n",
    "        predicted_sdgs = [self.metadata['target_names'][i] for i in range(len(y_pred[0])) if y_pred[0][i] == 1]\n",
    "        \n",
    "        return predicted_sdgs\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python predict_sample.py \\\\\"publication title\\\\\" [year] [cited]\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    title = sys.argv[1]\n",
    "    year = int(sys.argv[2]) if len(sys.argv) > 2 else None\n",
    "    cited = int(sys.argv[3]) if len(sys.argv) > 3 else None\n",
    "    \n",
    "    predictor = SDGPredictor()\n",
    "    if predictor.load_models():\n",
    "        predicted_sdgs = predictor.predict(title, year, cited)\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Predicted SDGs: {', '.join(predicted_sdgs) if predicted_sdgs else 'None'}\")\n",
    "    else:\n",
    "        print(\"Failed to load models\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the prediction script\n",
    "script_path = \"../predict_sample.py\"\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(predict_script)\n",
    "\n",
    "print(f\"   ‚úÖ Standalone script saved: {script_path}\")\n",
    "\n",
    "# 6. Final summary\n",
    "print(\"\\\\n6Ô∏è‚É£ Final Summary:\")\n",
    "\n",
    "print(f\"\\\\nüéâ PROJECT COMPLETION SUMMARY:\")\n",
    "print(f\"   Dataset processed: {len(df_processed):,} publications\")\n",
    "print(f\"   Features created: {combined_features.shape[1]:,}\")\n",
    "print(f\"   SDG classes: {len(mlb.classes_)}\")\n",
    "print(f\"   Best model: {final_best_name}\")\n",
    "print(f\"   Final performance: {final_best_score:.4f} F1-Score\")\n",
    "print(f\"   Models saved to: {models_dir}\")\n",
    "\n",
    "print(f\"\\\\nüìÅ Saved Components:\")\n",
    "print(f\"   - best_model.joblib (trained classifier)\")\n",
    "print(f\"   - tfidf_vectorizer.joblib (text vectorizer)\")\n",
    "print(f\"   - feature_scaler.joblib (numerical scaler)\")\n",
    "print(f\"   - multilabel_binarizer.joblib (label encoder)\")\n",
    "print(f\"   - kmeans_model.joblib (clustering model)\")\n",
    "print(f\"   - model_metadata.joblib (model information)\")\n",
    "print(f\"   - predict_sample.py (inference script)\")\n",
    "\n",
    "print(f\"\\\\nüöÄ Ready for Production:\")\n",
    "print(f\"   Use the SDGPredictor class for batch predictions\")\n",
    "print(f\"   Use predict_sample.py for single predictions\")\n",
    "print(f\"   All models are optimized and cross-validated\")\n",
    "\n",
    "print(f\"\\\\n‚úÖ NOTEBOOK EXECUTION COMPLETED SUCCESSFULLY!\")'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
